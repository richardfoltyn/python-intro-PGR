{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of machine learning (ML) algorithms\n",
    "\n",
    "***\n",
    "### Supervised learning\n",
    "\n",
    "  - Models of the type \n",
    "  $$\n",
    "    y_i = f(x_i)\n",
    "  $$\n",
    "  - For example, **linear regression model**\n",
    "  $$\n",
    "  y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\n",
    "  $$\n",
    "  - **Terminology:**\n",
    "\n",
    "    - $y$: dependent variable, response variable, outcome, target\n",
    "    - $x$: independent variables, features, covariates, predictors\n",
    "    - $\\epsilon$: error term\n",
    "    - $\\beta$: coefficients to be estimated\n",
    "\n",
    "  - **Examples:**\n",
    "\n",
    "    - Regression: continuous response $y_i$\n",
    "    - Classification: categorical response $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Unsupervised learning\n",
    "\n",
    "- \"unlabelled\" data, no response variables $y$\n",
    "- **Examples:**\n",
    "    - Clustering: find meaningful subgroups (clusters)\n",
    "    - Dimensionality reduction: keep only most relevant dimensions\n",
    "        - **Principal component analysis (PCA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Principal component analysis\n",
    "\n",
    "Consider the following scenario:\n",
    "\n",
    "- $N$ observations $i = 1, \\dots, N$\n",
    "- $K$ variables $\\mathbf{x}_k$, $k = 1, \\dots, K$\n",
    "\n",
    "Assume $K$ is large $\\Rightarrow$ we want to reduce dimensionality of problem:\n",
    "\n",
    "1. Keep only some subset of $K$ variables?\n",
    "2. Create **a few** alternative variables $\\mathbf{p}_j$ as linear combinations of $\\mathbf{x}_1,\\dots,\\mathbf{x}_k$ :\n",
    "$$\n",
    "\\mathbf{p}_1 = v_{11} \\mathbf{x}_1 + v_{21} \\mathbf{x}_2 + \\cdots + v_{K1} \\mathbf{x}_K\n",
    "    = \\sum_{k=1}^K v_{k1} \\mathbf{x}_{k}\n",
    "$$\n",
    "\n",
    "How do we pick coefficients $v$?\n",
    "$$\n",
    "\\max_{v_{11},v_{21},\\dots,v_{K1}} \n",
    "\\left\\{ \n",
    "    \\frac{1}{n}\\sum_{i=1}^N p_{i1}^2\n",
    "\\right\\}\n",
    "= \n",
    "\\left\\{ \n",
    "    \\frac{1}{n}\\sum_{i=1}^N \\left( \\sum_{k=1}^K v_{k1} x_{ik} \\right)^2\n",
    "\\right\\} \n",
    "\\quad \\text{subject to} \\quad \n",
    " \\sum_{k=1}^K v_{k1}^2 = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Singular value decomposition (SVD) and principal components\n",
    "\n",
    "- Data (feature) matrix $\\mathbf{X} \\in \\mathbb{R}^{N\\times K}$:\n",
    "    - $N$ observations in rows\n",
    "    - $K$ variables in columns\n",
    "- Assume that variables in $\\mathbf{X}$ are centred (mean zero)\n",
    "- The (compact) SVD of $\\mathbf{X}$ is given by\n",
    "    $$\n",
    "    \\mathbf{X} = \\mathbf{U} \\Sigma \\mathbf{V}^\\top\n",
    "    $$\n",
    "- $\\mathbf{U} \\in \\mathbb{R}^{N\\times K}$ and \n",
    "    $\\mathbf{V} \\in \\mathbb{R}^{K\\times K}$ are orthogonal matrices\n",
    "- $\\mathbf{\\Sigma} \\in \\mathbb{R}^{K \\times K}$ is a diagonal matrix\n",
    "    $$\n",
    "    \\mathbf{\\Sigma} =  \\begin{bmatrix} \n",
    "        \\sigma_1 & & & & \\\\\n",
    "        & \\sigma_2 & & & \\\\\n",
    "        & & \\ddots & & \\\\\n",
    "        & & & \\sigma_K & \n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "- Transform $\\mathbf{X}$ to get principal components:\n",
    "    $$\n",
    "        \\mathbf{P} = \\mathbf{X} \\mathbf{V}\n",
    "    $$\n",
    "- $j$-th column of $\\mathbf{P}$ is the $j$-th **principal component**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Example: Bivariate normal\n",
    "\n",
    "1. Construct sample of $N=200$ correlated observations from bivariate normal distribution\n",
    "2. Compute principal components using SVD\n",
    "3. Compute principal components using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Draw random sample\n",
    "\n",
    "1. Pick means and covariance matrix\n",
    "2. Generate random draws\n",
    "3. Plot random data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform PCA manually using SVD\n",
    "\n",
    "1. Demean (center) data\n",
    "2. Run SVD using `np.linalg.svd()`\n",
    "3. Apply transformation $\\mathbf{P} = \\mathbf{X}\\mathbf{V}$\n",
    "4. Plot original data vs. principal components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing PCA using scikit-learn\n",
    "\n",
    "1. Create instance of `sklearn.decomposition.PCA`\n",
    "2. Use `fit()` to compute principal components\n",
    "3. Use `transform()` to transform $\\mathbf{X}$ to principal components\n",
    "4. Useful attributes of fitted `PCA` object:\n",
    "    - `n_components_` stores number of PCs used\n",
    "    - `components_` contains matrix $\\mathbf{V}^{\\top}$\n",
    "    - `explained_variance_` stores the variances of all principal components; and\n",
    "    - `explained_variance_ratio_` stores the fraction of the variance \"explained\" by each component.\n",
    "5. Compute loadings (rescaled coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Example: Higher-dimensional data\n",
    "\n",
    "#### Creating highly correlated inputs\n",
    "\n",
    "1. Draw $N$ independent samples from a bivariate normal distribution:\n",
    "    $$\n",
    "    \\mathbf{z}_i \\stackrel{\\text{iid}}{\\sim} N\\left( \\mathbf{0},\n",
    "        \\begin{bmatrix} \\sigma_1^2 & 0 \\\\ 0 & \\sigma_2^2 \\end{bmatrix}\n",
    "    \\right)\n",
    "    $$\n",
    "    and stack them in the matrix $\\mathbf{Z} \\in \\mathbb{R}^{N\\times 2}$.\n",
    "2. For some $2 \\times K$ matrix $\\mathbf{A}$ with $K \\gg 2$, we compute\n",
    "    $$\n",
    "    \\mathbf{X} = \\mathbf{Z} \\mathbf{A}\n",
    "    $$\n",
    "    which gives us the higher-dimensional matrix $\\mathbf{X} \\in \\mathbb{R}^{N\\times K}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualise data, compute correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA with manually selected number of principle components\n",
    "\n",
    "1. Bivariate scatter plots vs. first principal component\n",
    "2. Scree plot of explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA with automatically selected number of principal components\n",
    "\n",
    "1. Fit PCA with `n_components=0.9`\n",
    "2. Plot loadings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py3-default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89692ee4bd8d7a7842eb7c7050f10ae8c4113955275de7625334d5364ea86119"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
